---
title: 上帝视角看GPU
date: 2023-04-16 00:55:00
updated: 2023-04-16 00:55:00
id: ml-20230416-005500-g184
categories:
	- Unity
	- TA
tags: 
	- Unity
	- TA
	- Shader
	- GPU
---

专注于GPU本身

<!--more-->

# 图形流水线基础

## 显示

`显示器`以`像素`为基本单元显示画面  
常见的情况下, 每个`像素`包含红绿蓝, 即RBG三个分量, 每个分量8位, 即0-255区间  

> 随着时间推移, 有些显示器可以使用更多位表示颜色, 但具体的细节还是类似  
> > 本次以8位做讲解

显示器每次从`帧缓冲区(Frame Buffer)`读取等待显示的像素列表  
这意味着帧缓冲区的内容与显示器的显示内容数据结构上是一致的  
由于32位处理更快速, 所以一半有RGBA每个8位, 共计32位, 即使`A(Alpha)`会被显示器忽略

> 此技术主要用于防止以避免屏幕被撕裂, 即用户看到刷新到一半的内容  
> > 扩展阅读 `垂直同步`

此时就有了原始的显卡(Graphics Card), 即图像的搬运工, 将帧缓冲区的内容发送给显示器  
仅仅只是一个图像的搬运工

## 图像处理

一个新的需求诞生了, 需要将图像的RGB都乘以2之后显示  
固然可以在CPU上给每个像素值操作, 但性能上更好的方法是在搬运图像前新增一个像素处理单元

灵活多变的需求催生出了`Shader`, 此处是`PixelShader`  
这种可添加Shader的单元叫做`可编程流水线单元(Programmable Pipeline Unit)`  
每个Pixel仅处理一个像素, 单入单出, 无疑拥有更高的效率

此时, PixelShader的参数是一个坐标, 依据坐标从图像上采集颜色, 执行操作后返回
> 具体参数的使用方式, 取决于程序的编写者

输入的图像称作`纹理(Texture)`

由专门的硬件负责从内存中读取纹理, 运行Shader代码, 存放到帧缓存区域  
这便是针对图像的处理器, 可称为`Picture Processing Unit`

## 几何网格(模型)处理

将简单网格(点, 线, 三角形), 称之为`图元(Primitive)`  
存放`顶点(Vertex)`的`Buffer`称为`Vertex Buffer`  
链接顶点的线存储为一个整数, 表示Vertex的索引, 称其`Index Buff`

将这些顶点和线组装成三角形, 然后就可以开始处理光栅化的流程  
裁剪屏幕外三角形, 计算三条边的方程等  
然后送入`Rasterizer`  
此步骤称为`图元组装单元(Primitive Assembler)`

同一个几何体, 可以变换位置与角度.  
因为几何本身的拓扑关系不变, 所以被存储的顶点和线也不会改变  
这里就涉及到变换, 将几何体从几何空间变换到屏幕空间  
即将`(x,y,z,1)`变换为`(x',y',z',w')`

* World Matrix
  * 位置, 朝向, 缩放
  * 世界坐标
  * 将物体放置与一个全局的世界中
* View Matrix
  * 摄像机的位置
  * 摄像机坐标, 又称观察者坐标
  * 将物体放置在以摄像机为原点的坐标系中
* Projection Matrix
  * 摄像机参数, 入视野宽窄, 视域远近
  * 投影, 投影矩阵, 投影坐标
  * 将物体显示在屏幕范围空间, 并带有近大远小

很显然, 这是固定不变的算法, 也适合使用硬件处理解决  
称之为`硬件变换与光照(T&L)`  
又称`Transforming以及Lighting`, `光影转换`

后续由于需求多样性, 也开放了可编程的部分`VertexShader`  
即对每个顶点的处理, 顶点信息有多种保存方式, VertexShader也无需知道格式  
需要调用者提供一个格式的表述, 比如`单缓存布局(Single Buff Layout)`, `多缓存布局(Multiple Buffer Layout)`  
当然, 最后由一个固定的流水线单元, 即硬件部分, 称为`Input Assembler`  
从VertexBuff里组装出一个顶点, 动刀VertexShader处理

如此, 每个VertexShader也是单入单出的模式

自此以得到一条基本的`图形流水线(Graphice Pipeline)`  
调用者提供输入的信息后, 就能流程化的输出图像, 并送给显示器  
流水线中由可编程带院和不可编程单元组成, 保证效率的同时提供一些灵活性

这便是2000年代主流GPU的构成  
这将CPU从复杂大量的操作中解放出来  
由专业的`GPU(Graphics Processing Unit)`来并行处理

## 小结

可以看到不论是PixelShader还是VertexShader都用于接受并处理数据  
他们并不关心如何从内存读取, 处理完毕后如何写出, 前后都有固定的流水线来处理  
看起来, Shader就像回调函数, 由GPU在需要处理大量数据时, 每个单元调用一下

此时, 可以看到CPU和GPU的差别了  
CPU擅长在小数据上做相对复杂的串行处理和逻辑操作  
GPU擅长在大量数据上做相对简单的并行处理  
这些特点使得他们可以分工协作, 更有效率


# 逻辑上的模块划分

## 细分

随需求又有新的`GeometryShader`, 相当于拆开`图元组装单元(Primitive Assembler)`  
此Shader可以单入多出, 即一个Primitive变为多个Primitive  
由此可将三角形移动或切成多个(自定义个数)  
开启了GPU可处理非均匀输入输出的任务先例  
但此Shader并非必须, 可退化为Primitive Assembler  

然后, GeometryShader可以直接导出数据到内存, 这也相等于其前置工序`Early Primitive Assembler`也同时拥有这项能力  
提供了从流水线中直接导出数据的能力  
典型的应用场景是, 存储中间数据, 减少重复计算

但是使用后发现, 性能是很低的  
因为灵活, 硬件难以做出针对性优化, 实现的非常保守  
硬件在执行之前甚至不知道你要处理的是三角形

于是, 随着三角形细分需求的增加, 逐渐衍生出在VertexShader之后的`Tessellator`工序  

* `Hull Shader` 指定图元如何细分
  * 内部分出多少分
  * 每条边分成多少段
* `Tessellator`
  * 细分算法
* `domain Shader` 计算细分后的顶点信息

这个工序也是可选的

## 通用计算

于是之后GPU做通用计算的能力开始大幅发展  
最早是先渲染一个覆盖屏幕的大三角形, 在PixelShader内做并行计算  
但是这还是单入淡出, 且需要经过整条流水线, 存在浪费, 而且学习成本也很高  
此方向称为`GPGPU`, 即`通用图形处理器（General-purpose computing on graphics processing units)`

后续催生出可多入多出, 可任意读取写入的, 不经过流水线的单元`Comppute Shader`  
独立于流水线单独存在

## 自动生成

后续优化方向是, 少数入甚至不输入数据, GPU可以自行生成顶点  
于是有了`Amplification Shader`, `Mesh Shader`  
其中AmplificationShader负责指定执行多少次MeshShader  
MeshShader负责产生几何体  
此时渲染的不再是图元, 而是一小块网格, 称为`meshlet`

## 光线追踪

再后面就是光线追踪, 这是一条独立的流水线, 包含多个新的Shader  
* `Ray Generation Shader` 生成光线
* `Intersection Shader` 判定光线与物体是否相交
* `Any Hit Shader` 光线打到物体上时是否可继续前进
* `Closest Hit Shader` 光线打到物体的最近点计算颜色
* `Miss Shader` 光线未打到任意点时计算颜色
* `Callable Shader` 动态调用

## 更多流水线

同样的流水线扩展思路, 还可用于更多领域
* `Tensor Core` 神经网络计算, 张量核
* `Video Codec` 视频编码解码
* 等...

## 更多展望

CPU是一个通用模块  
GPU则有多条专用流水线, 需要有具体了解才能再程序中开发  
目前GPU各个流水线不能互相调用, 如果有这种需要, 则需经过内存中转

目前GPU有区分为图像计算(GPU)和神经网络计算(GPGPU)  
实际上图像计算单元也可作为通用计算单元的高效补充  
比如Rasterizer可作为高效的插值器  
OutputMerger可作为高效的数据累加器

> GPU连连看是否可能实现? 硬件层面如何处理

## 小结

本节讲解了GPU多重流水线的扩展过程  
但是目前有如此之多的Shader, 很容易遇到负载均衡问题  
而且流水线复杂度提高, 硬件的生产成本也是个问题

# 部署到硬件

部署到硬件, 则需要考虑成本, 性能, 功耗

## 负载均衡

早些时候的显卡, 一般有完整的Shader单元区分  
如`GeForce FX 5600`, 有2个VertexShader和4个PixelShader  
如果像素和顶点的分布并非这个比例, 那么很难发挥它全部的效率  
这引申出了第一个需要解决的问题, 负载均衡

2005年前, 一般普遍认为VertexShader需要`32位浮点精度运算(FP32 ALU)`, 因为坐标需要这个精度, 但不需要读取内存    
而PixelShader则相反, 只和颜色打交道, 需要读取内存`采样器(Sample)`, 但只需要`低精度运算器(Low precision ALU)`

但随着需求发展, 界限变得模糊, 大规模地形渲染需求, 使得VertexShader可以读取纹理  
PixelShader也更多被用于通用计算领域, 需要更高精度的运算

结果就是两个单元拥有同样的采样器和运算器  
于是有了`统一的Shader架构(Unified Shader Architecture)`  
即在流水线中的两个不同阶段, 实际上使用同一种硬件执行  
要达到这种动态分配机制, 需要新增`调度器(Scheduler)`  
如此便解决了一个基础的动态负载均衡问题

继续分解, 一个Shader内纹理采样的次数远远低于计算次数  
即采样器可以少于运算器  
这里也拆出来共享给调度器分配

> 其他固定流水线也大多是这种方式安排

# CPU与GPU的主要差异, 并行处理

例如RTX3090拥有
* 10752个`统一Shader核(Unified Shader Architecture)`  
* 336个`采样器(Sample)`
* 112个`光栅化和像素操作单元(ROPs)`

那么为什么GPU能有上万个核心, 而CPU几十个核心就已经非常多了  
先提及两个基本概念, `单指令单数据流(SISD)`, `单指令多数据流(SIMD)`  
CPU主要是SISD, 少量配有SIMD  
而GPU则主要是SIMD, 实际上是一条指令同时处理多分数据

CPU上存在的核称为`流处理器(StreamingProcessor, 即SP)`  
很多个SP加上一个`控制器(Controller)`就能组成一个相对完整的`流式多处理器(StreamingMultiProcessor,即SM)`  
RTX30系列GPU, 一个SM包含128个SP,4个采样器  

可以看到CPU和GPU核心的功能并不完全相同, 主要是GPU计算更多单控制更少  
所以填不满所有的SP是性能浪费的一个重点问题, 需要考软件方面弥补, 即分配任务的机制

因为一个控制器实际控制很多SP, 所以GPU处理各种分支时, 实际上是运算了两次, 再分为两次执行, 即消耗双倍时间和性能(或多倍)  
这里想要使用每个SP单独控制, 则成本和复杂度太高  
后续优化为每个SP只执行一个分支的代码, 然后每批相同的代码同时运行, 如此只消耗时间, 不消耗更多计算性能

如果计算是多数据同步进行的, 那么就需要很多的数据读写能力, GPU拥有自己的显存, 要比CPU的缓存大得多  
一般显存位宽会高于内存, 也是为了读写更多数据  
显存实际上就是高延迟换取高吞吐量  
这实际上也影响到了GPU的效率, 因为每次读写需要更多的等待时间  
所以有组件名为`调度器(Warp)`, 每次因为读写等待时, 实际会切换到其他的Warp来执行, 直到数据等待完毕, 再继续, 保证等待期间仍能继续发挥性能  
每个Warp会占用一些`寄存器(RegisterFile)`的空间来存储局部变量, 如果寄存器占满, 则无法再运行Warp, 必须等待现有Warp执行完毕  
这里也可以优化数据大小来提升寄存器效率

## 小结

可以看出CPU的延迟小, GPU计算吞吐量大

再回看成本, 性能, 功耗, 硬件的制造必须考虑这些

# 完整的软件栈

如何从软件控制GPU。常见的API、驱动，都是什么

## 结构演化

在很久以前, 程序需要通过操作系统提供的硬件操作接口直接读写,操作图形硬件  
```plantuml
start
:APP;
:OS;
:GPU;
stop
```
如果每个程序都自己实现硬件调用操作, 可想而知开发效率非常低

之后采用抽象思路, 形成公共的接口层`应用程序编程接口(Application Programming Interface, 即API)`  
程序调用接口层, 然后底下执行负责
```plantuml
start
split
:APP1;
split again 
:APP2;
split end
:API;
split
:OS1;
:GPUDriver1;
:GPU1;
split again 
:OS2;
:GPUDriver2;
:GPU2;
split end
stop
```
程序只需针对图形API写一遍即可兼容各种操作系统和硬件

> 但实际上各个操作系统和硬件的实现各有细节上的差异, 但比起直接操作硬件, 效率还是提高很多的

其中`GPUDriver`由不同的硬件厂商提供, 翻译成具体硬件操作  
可想而知GPUDriver这里也有很多重复实现, 于是抽象成一个`设备驱动接口(Device Drive Interface, 即DDI)`, 他负责这些重复的部分, 而下面的GPUDriver负责硬件的特殊部分

即形成如下层次翻译的结构(通过接口联系并翻译)
```plantuml
start
:APP;
:OS;
:DDI;
:GPUDriver;
:GPU;
stop
```

> 当然, 这些都是理想的大统一情况, 实际上操作系统和硬件厂商各有不同, 其中细节也有差异

## 现实架构举例

### 微软的Direct3D(D3D)

这里只讨论Windows官方实现, 即不跨平台(仅Windows), 但跨厂商(各GPU厂商支持)

在WindowsXP时代, 架构基本和理论一致, 厂商提供GPUDriver即可

然而随着稳定性, 性能, 共享资源的需求不断增加, 到`Windows Vista`时代, Runtime和厂商驱动都进一步分为用户态(UMD)和内核态(KMD)两种, 并分别拥有自己的DDI

```plantuml
start
:APP*N;
:API;
:D3DRuntime;
:UMD DDI;
:UMD;
:DXG Kernel;
:KMD DDI;
:KMD;
stop
```
这称为`Windows显示驱动模型(Windows Display Dirver Model, 即WDDM)`
其中UMD和KMD由厂商提供, 由于细化了使用场景, 抽离了更多公共接口, 使得厂商开发难度降低, 不同厂商的差异性减少, 稳定性提升

多年来, D3D推出了多个版本, 常用其中的9, 11, 12
这些UMD层都不兼容, 甚至需要重写, 但下面的KMD总体是不变的


### Khronos的OpenGL

跨平台, 跨厂商

Khronos是一个标准协商会议, 具体有什么API其实是各家厂商在会议上协商出来的

其中Windows为其提供了`可安装用户驱动(ICD)`, OpenGL在此层实现了自己的UMD  

到了Linux, OpenGL有更灵活的实现方式
* 厂商完全实现整个OpenGL
* 基于Mesa3D框架提供的`Gallium3D DDI`来调用厂商的`Gallium3D Driver`
  * 后续扩展出对于OpenGL ES, Vulkan等API的支持

> OpenGL的版本基本都是向下兼容的

### NVIDIA的CUDA

跨平台, 不跨厂商

提供了对于D3D, OpenGL, OpenGL ES, Vulkan的兼容, 使其能运行于Windows和Linux

多用于存粹计算API(有限元模拟, 神经网络训练等), 但也可以调用图形渲染

由于不跨厂商, CUDA可以提供一些图形API内没有的功能, 比如ShaderMemory概念, 用于提升GPGPU的运算效率

> ComputeShader也受CUDA的影响而设计

### 小结

硬件厂商提供的API, 有新功能之后, 可以很快的自底向上提供支持  
OpenGL这类讨论后提供的API, 也是自底向上模式, 支持速度也较快, 而且提供扩展机制, 无需更新API即可提供扩展支持, 只需要厂商提供一个扩展说明, 其他厂商如果也对这个API感兴趣, 也可以跟进支持, 最后变为新版本的公认API, 等于是先做灰度发布  
而D3D这类平台APi, 则是自顶向下的设计, 需要更长的时间来进行版本更新支持, 新功能的添加必须依赖于版本更新, 所以功能的支持总是慢半拍

总览时间轴, 发现各个API都在让出更多的自由行动, 交给程序实现, 因为程序自己知道自己的意图, 但这样又回归了直接操作硬件的问题, 多个程序需要多个实现, 于是还诞生了渲染引擎的抽象层.

## API不支持的情况

如果某些硬件确实不支持API中的一些功能, 实际上可以提供软件模拟GPU的实现  
虽然效率极低, 但是有效的实现

而且跨平台的驱动并不是完全复刻, 需要厂商提供,甚至重写  
比如高通的AdrenoGPU, Android上支持OpenGL ES, Windows支持D3D, 这是由于提供了两份驱动, 并不能以为在Windows仍然支持OpenGL ES, 因为没有提供相关的驱动实现

## 小结

实际上, 整个架构做的事情就是层次提供接口, 层层向下翻译  
所以每个层实际上并不一定需要给下一层执行, 可以由另外实现了接口的内容执行  
比如
* `ANGLE`将OpenGL ES翻译为D3D, OpenGL, Vulkan
* `MoltenVK`吧Vulkan翻译为Metal提供iOS支持
* 比如`D3D11On12`, 实际是用D3D12提供了一个D3D11的UMD层, 程序调用D3D11的API, 实际到UMD之后, 是由D3D12的API来再次执行

然后还可以提供存软件形式的模拟GPU, 即在CPU上做所有的内容  
可以用来做模拟测试, 或临时使没有GPU的机器使用GPU的一些功能之类的  

更有甚者, 驱动可以不让本机的硬件执行, 而是发送硬件操作到另一台机器执行  
即云GPU, 虚拟GPU

## 更多扩展

程序调用API, 硬件执行后就可以到帧缓存区域等待渲染了吗  
并不一定, 同一个硬件中, 多个不同的软件同时想要渲染, 到底谁先谁后, 写入哪个地方, 如何保证不冲突呢  
这里有一个`合成器(Compositor)`, 比如Windows的DWM, Android的SurfaceFlinger

每个软件将自己的渲染结果写入纹理,存储到内存, 并交给合成器处理  
合成器还会组合这些纹理, 再次调用图形API, 才送入帧缓存  
这就是窗口模式, 每个程序有自己的窗口, 可以有操作系统切换

> 有的软件有全屏独占模式, 实际就是绕过了合成器, 直接输出覆盖帧缓存, 稍微可以提高一些性能, 但会让其他的软件无法渲染

实际一些窗口动画(放大缩小, 选择高亮), 窗口特效(毛玻璃, 半透明),  都是合成器在处理

# 图形流水线里的不可编程单元

## 光栅化器

光栅化器Rasterizer是GPU在实时渲染方面的优势  
以至于很多时候光栅化就是图形流水线的代称

经过VertexShader后, 每个顶点都有了转化后的属性  
然后图元组装单元(Primitive Assembler)后, 生成对应三角形
光栅化,本质上就是将三个顶点的信息插值到三角形内(三角形覆盖)每个像素上

由于PrimitiveAssembler和Rasterizer都是硬件操作且连续在一起  
所以也被是为广义上的光栅化器

插值的常见算法称为扫描线算法  
根据给定的顶点数据和位置, 可以算出三角形内一个像素左移或下移各个属性会改变多少  
称其为`ddx`,`ddy`

那么, 像素在什么情况下是为被三角形覆盖, 即光栅化规则(Rasterizer Rules)
* 普通模式, 即像素中心是否在三角形范围内
  * 对线, 则看其是否经过像素内一个菱形区域
* 保守式光栅化(Conservative Rasterization), 即沾到一点就算被覆盖
  * 常用于体素化需求

## 硬件实现

直接放到硬件上就成为`立即式光栅化(Immediate Mode)`  
即算法直接硬件上连线, 计算效率非常高, 但算法固定  
对于大三角形, 效率更好, 因为ddx和ddy只需计算一次, 后续累加即可  
在硬件上不被打断的连续执行同样的操作

如果三角形重重叠叠, 则需要反复写入, 带宽占用大, 功耗高  
对于PC, 性能好, 功耗高, 正是首选  
对于移动平台, 则更看重性能功耗比, 性能只有一半, 但功耗只有1/4也会考虑选用  
于此, 移动平台往往采用`Tile-Based`, 即将渲染目标划分为一些很小的Tile, 比如32*32, 每个Tile包含一个列表, 存储和这个Tile相交的所有三角形  
所以可以一批批的处理三角形, 但这样就需要有一个片上内存充当cache的角色, 访问速度要快  

对于每个Tile, 将渲染目标区域载入片上内存, 然后执行光栅化, 写回结果  
如此反复直至所有Tile处理完毕

不论三角形如何层叠, 每个Tile只对内存需读取32*32, 功耗更低, 但每个片的渲染需要经常打断三角形渲染, 总体性能其实也相对降低了

对于立即式, 无论渲染两个三角形分别到两张纹理, 还是都渲染进一张纹理, 实际都是跑两次光栅化, 性能差异忽略不计

对于TileBase则不同
* 第一个Tile载入到片上内存, 渲染两个三角形, 存到纹理..如此往复
* 第一个Tile载入到片上内存, 渲染一个三角形, 存到纹理..如此到第一个三角形完毕, 则开始第二个三角形的渲染

可见, 对于输出纹理的不同, 需要多次和内存进行读写操作, 性能是大幅度降低的  
> 注意, 访问内存比访问片上缓存, 需要的时间, 电量等.. 都要多的多

所以在TileBase的GPU上, 切换渲染成为大忌

所以最近的API的改进, 让开发者自行决定是否吧纹理的Tile载入片上内存, 选然后是否存储到内存, 用于减少这部分的开销功耗

## 性能优化

光删化处理完毕后, 需要传递给PixelShader, 然后传给OutputMerger  
深度测试是在OutputMerger中完成的, 所以遮挡关系到这里才被处理, 之前PixelShader渲染的像素就要丢弃  
所以就有了可以将深度测试放到PixelShader之前的`Early-Z`  
但注意, PixelShader也可以输出深度信息和透明度(透明混合), 并可以丢弃像素  
没有这些处理的PixelShader才能进行Early-Z

> 为什么不在光栅化前就做丢弃, 因为光栅化插值有精度问题

有的GPU会有称为`Tile-based Deferred Rendering(TBDR)`的模式  
吧位置和像素颜色都记录片上内存, 遮挡则直接覆盖, 但也需要满足一些条件才能启用

立即式和TileBase的结合, 称为`Tiled-Caching`  
Tile变大, 256*256这个级别, Cache也变大, 可以载入几何  
如此降低了内存访问频率, 性能和功耗表现都更好  
> 代价是硬件成本

## 软件实现替代硬件实现

在硬件上执行光栅化, 拥有固定硬件优势, 其性能非常优秀  
但市面上仍一些有软件光栅化算法, 提供更多灵活性

比如小三角形光栅化, 硬件对于三角形内像素光栅化, 并非一个一个执行, 而是一个一个Quad执行, 如2*2, 这导致, 对于小三角形, 有更多光栅化的工作量需要被丢弃
> 小于一个像素, 则浪费3/4

UE5的nanite就是做了小三角形的软件光栅化优化

另一个需求是无法使用硬件光栅化时, 可以使用软件光栅化  
比如早期之前Inter的一些GPU(Larrabee项目), 实际上是多个奔腾处理器加上宽指令集并行工作, 就需要软件处理光栅化

> 可见开源项目`SalviaRender`渲染器

NVIDIA也有类似的工作, 比如用CUDA构建软件光栅化

光栅化器这种硬件, 即不可变流水线也可以被软件替代, 那么其他硬件流水线呢  
* Input Assembler
  * 负责读取和组装顶点
  * 在Shader可以直接访问Buffer之后, 这很容易变为Shader直接处理
    * 在位置和法线使用不同Index的情况下, 性能更好
    * 比如虚幻引擎的`ManualVertexFetch`
* StreamOutput
  * 原理相同
* Tessellator
  * 三角形内生成新的顶点, 并组成新三角形(曲面细分)
  * 这也可以使用算法实现, 在Comppute Shader实现
    * 可提供更灵活可定制的细分算法, 或可更高性能
    * CivilizationV使用此方式做地形细分
* OutputMerger
  * 很明显可代码化
  * 注意AlphaBlending没有硬件帮助, 性能有影响

唯一对于硬件有优势的是, 纹理采样器(Sampler)  
他需要对于纹理进行读取, 解压, 插值, 计算量很多, 算法也固定

Larrabee研究结论, 软件实现的采样器性能降低12-40倍

所以对于所谓的计算型GPU, 也可以通过软件形式做渲染流水线  
在驱动中被调用, 上层无感知 

## 小结

GPU的硬件架构目前还在持续优化, 各种软件算法也都在发展, 后续可能还有更多新变化

# 光线跟踪流水线

最简单的光线追踪算法, 称为`光线投射(Ray Casting)`  
从摄像机原点, 向每个像素发射一束光线, 一步步跟踪直到光线命中物体  
命中物体后, 依据光源和物体的材质计算该像素的颜色  
这能得到`直接光照(Direct Lighting)`, 即像素颜色来自光源的直接贡献  
1992年的德军总部3D就使用了此算法

较低分辨率下跟踪形状固定的物体, 有还行的效果, 性能也过得去, 不用Z-Buffer也能有正确的遮挡关系

光栅化渲染时, 需要解答物体占据那些像素  
光线投射时, 需要解答沿这条光线方向能看到(命中)什么物体

这里可见, 两者的渲染流程完全不同  
主要的区别就是, 光线跟踪需要有完整的场景概念  
因为光线发出的时候, 并不知道会命中哪里, 所以需要完整的场景和物件, 正确的位置关系, 全部提交给GPU, 渲染的单位从三角形变为场景

更复杂的, 当光线命中物体表面时, 还可以依据算法进行反射, 漫反射, 折射, 散射等.. 生成多条新的光线继续跟踪, 直到命中光源为止  
最后从光源反向计算每次命中物体的颜色, 从而得到像素颜色值  
这便是`光线跟踪(RayTracing)`

由于已经有了场景内全部的数据, 自然就达成了实时渲染中梦寐以求的`全局光照(GI, Global Illumination)`  

不是直接来自于光源, 而是来自于其他光路间接贡献的光照, 称为间接光照  
光线追踪前, 为了计算间接光照, 反射, 漫反射, 折射, 各有各的方法, 而且很多互相有冲突  
有光线追踪后, 这些都可以用统一, 一致的方法完成, 效果还更好

不论光栅化, 还是光线跟踪, 物体都可以使用三角形的几何表示  
光线追踪的过程, 一定会涉及到光线和三角形的几何求交, 如果每个三角形于每一束光线都进行计算, 那肯定是效率低下 
长期以来, 研究的重点就是尽早的吧不会相交的情况排除, 以加速跟踪  
常用的是将物体放入一个树形结构, 如果已经不与一个子树相交, 就不会于其下的任何节点相交, 即排除掉整颗子树(剪枝)

光线追踪领域常用的`加速结构(Acceleration Structure)`叫做`BVH`, 其叶子节点是物体本身 , 物体的包围盒之间的关系, 往上组成一级一级的树.  
一堆三角形可以显示组合成物体, 称为显示物体  
也有过程式物体, 称为隐式表达, 给与一个包围盒和一个隐式函数, 通过传入坐标, 函数告知是否存在于物体上  
此两种物体表达, 都可以用于光线跟踪

> 光栅化在应用阶段视锥剔除也用BVH

在硬件实现上, 需要先对场景中的物体建立底层加速结构, 然后组成场景本身的顶层加速结构  
然后, 场景中有很多动态移动的物体, 如果每次移动都需要重新构建加速结构, 则开销太大, 所以需要支持局部构建, 增量更新  

加速结构将被提交到GPU  
从Ray Generation Shader出发, 生成光线, 然后遍历加速结构, 找到可能有相交的叶子节点, 如果是三角形物体, 则与三角形求交, 如果是过程式物体, 则于包围盒求交,并调用Intersection Shader, 来确认是否真的于物体相交  
如果命中, 则转向Any Hit Shader, 判定是否继续跟踪  
光线打到物体的最近点, 会调用一次Closest Hit Shader来计算颜色  
未命中的则使用Miss Shader

由于光线出发前并不知道自己命中什么物体, 所以需要将场景中所有的Shader都放入一张表中, 方便GPU依据命中物体的ID, 动态查找使用哪个Shader  
这和光栅化不同, 光栅化之前就可以决定使用哪种Shader  
光线跟踪的Shader有动态调用的能力, 称为Callable Shader

这些仍然是在Unified Shader Architecture单元中执行, 其中的RTCore就是做的求交的加速操作

但是API只是定义接口, 经过API后, 厂商可以多种方式实现, 只要符合接口定义即可  
应用程序只需将场景扔进API, 能构建出加速结构, 再将加速结构扔进API, 得到渲染结果即可.

以后有更好的结构或实现, API下的实现有可能改变

甚至由于是API定义, 所以这里也可以使用纯软件实现  
对于GPGPU, 也可以使用软件算法在驱动中给与光线追踪相关的支持, 使用起来也和正常GPU没什么不同(性能不同)

参阅github`D3D12RayTracingFallBack`. 完全没有硬件的情况下也能运行管线追踪

`Imagination(Imagination Technologies)`提供了一些光线追踪的指导思想

* Live-0 Legacy Solutions
  * 史前时代的光线跟踪(Legacy Ray Tracing)
  * CPU, 专用卡, 可编程, 不可变成, 实现方式不统一
  * 大概都是类光栅化的结构
* Live-1 Software on Traditional GPUs
  * GPGPU的光线跟踪(GPGPU Ray Tracing)
  * 实现可编程单元的GPU硬件支持, 包含加速结构和跟踪框架
* Live-2 Ray/Box and Ray/Triangle Testers
  * 硬件求交测试(Hardware Intersection Testings)
  * 可硬件执行光线相交检测
    * 相比可编程的相交检测, 芯片面积缩小44倍
  * 进入实用阶段
* Live-3 Bounding Volume Hierarchy(BVH) Processing in Hardware
  * 硬件BVH操作
  * 遍历BVH
* Live-4 BVH Processing with Coherency Sort in Hardware
  * 硬件的BVH操作的相关性排序
  * 排序可以将相同或相似的分支同批次执行, 大幅度提升效率
    * 软件排序实际上也可以显著提升性能
* Live-5 Coherent BVH Processing with BVH Hardware Builder
  * BVH可以在硬件上构建和更新
    * 目前有实验版, 还没有实用化的产品

由于目前光线跟踪的性能问题, 游戏中多是做混合操作, 即部分跟踪, 部分光栅化, 通常还提供开关选项, 将光线跟踪作为一个提升体验的可选项

比如用光栅化生成直接光照, 用光线跟踪生成一次间接光照和阴影  
降低跟踪分辨率  
使用连续的帧信息减少计算, 以空间换时间

> UE5的lumen就可以使用硬件光线跟踪作为软件的补充, 以较少代价提升间接光照的效果

未来光线跟踪性能提升, 或显卡性能提升后, 还会出现如同通用Shader一样的, 通用光线追踪单元, 用于做通用计算, 比如物理模拟, 碰撞检测等..

# 参考图

![][上帝视角看GPU1]
![][上帝视角看GPU2]
![][上帝视角看GPU3]

# 完毕

**感谢您的观看!**  
本文来自 [ML-Blog][ML-Blog_Link]

<!-- 图片 -->
[上帝视角看GPU1]:https://github.com/UserMingHaoLi/ML_HexoBlogContentImages/blob/main/Content/Unity%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF/29GPU%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF-%E4%B8%8A%E5%B8%9D%E8%A7%86%E8%A7%92%E7%9C%8BGPU1.jpg?raw=true "上帝视角看GPU1"
[上帝视角看GPU2]:https://github.com/UserMingHaoLi/ML_HexoBlogContentImages/blob/main/Content/Unity%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF/29GPU%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF-%E4%B8%8A%E5%B8%9D%E8%A7%86%E8%A7%92%E7%9C%8BGPU2.jpg?raw=true "上帝视角看GPU2"
[上帝视角看GPU3]:https://github.com/UserMingHaoLi/ML_HexoBlogContentImages/blob/main/Content/Unity%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF/29GPU%E6%B8%B2%E6%9F%93%E7%AE%A1%E7%BA%BF-%E4%B8%8A%E5%B8%9D%E8%A7%86%E8%A7%92%E7%9C%8BGPU3.jpg?raw=true "上帝视角看GPU3"
<!-- 链接 -->

<!-- 水印 -->
[ML-Blog_Link]:https://userminghaoli.github.io/ "我的博客"

